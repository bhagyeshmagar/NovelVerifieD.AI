================================================================================
                    NOVELVERIFIED.AI - PROJECT DOCUMENTATION
                    Team StrawHats - KDSH 2026 Track A
================================================================================

LAST UPDATED: January 11, 2026

================================================================================
SECTION 1: PROBLEM STATEMENT SUMMARY
================================================================================

TASK: Verify whether hypothetical character backstories are CONSISTENT or 
      CONTRADICTED by the actual novel text.

INPUT:
- Full novel text (100k+ words, no truncation)
- Hypothetical character backstory (early-life events, beliefs, motivations)

OUTPUT (results.csv):
- Story ID: Claim identifier
- Prediction: 1 (consistent) or 0 (contradicted)
- Rationale: Short explanation (1-2 lines)

TRACK A REQUIREMENTS (What we're following):
1. MUST use Pathway framework for at least one part of the pipeline
   - Ingesting narrative data ✓
   - Storing/indexing novels
   - Retrieval over long documents
   - Document store/orchestration

2. CAN use any modeling approach:
   - Transformer-based LLMs ✓ (Claude API / Ollama)
   - Agentic pipelines ✓ (7-agent architecture)
   - Classical NLP pipelines
   - Hybrid approaches
   - Rerankers, classifiers, heuristics

EVALUATION CRITERIA:
- Accuracy on classification task
- Novelty in NLP/GenAI methods (beyond basic RAG)
- Handling of long context (chunking, retrieval, coherence)

================================================================================
SECTION 2: CURRENT ARCHITECTURE
================================================================================

Our system uses a 7-AGENT PIPELINE:

┌─────────────────────────────────────────────────────────────────────────────┐
│                              DATA SOURCES                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│   data/novels/*.txt          │         data/train.csv & test.csv            │
│   (Full novel texts)         │         (Claims to verify)                   │
└─────────────────────┬────────┴───────────────────┬──────────────────────────┘
                      │                            │
                      ▼                            ▼
┌─────────────────────────────┐    ┌─────────────────────────────┐
│    AGENT 1: INGESTION       │    │    AGENT 3: CLAIM PARSER    │
│    (ingestion_agent.py)     │    │    (claim_parser.py)        │
│    - Uses Pathway framework │    │    - Reads train.csv/test.csv│
│    - Chunks novels          │    │    - Extracts claims        │
│    - 1400 tokens/chunk      │    │    - Outputs claims.jsonl   │
│    - 300 token overlap      │    │                             │
└─────────────────────┬───────┘    └─────────────────┬───────────┘
                      │                              │
                      ▼                              │
┌─────────────────────────────┐                      │
│    AGENT 2: EMBEDDING       │                      │
│    (embedding_agent.py)     │                      │
│    - Uses sentence-transformers                    │
│    - Model: all-MiniLM-L6-v2│                      │
│    - Creates FAISS index    │                      │
└─────────────────────┬───────┘                      │
                      │                              │
                      ▼                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                       AGENT 4: RETRIEVER                                     │
│                       (retriever_agent.py)                                   │
│    - For each claim, finds relevant novel chunks                            │
│    - Semantic search using FAISS                                            │
│    - Returns top-k most relevant passages as evidence                       │
└─────────────────────────────────────────────────────┬───────────────────────┘
                                                      │
                                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                       AGENT 5: REASONING                                     │
│                       (reasoning_agent.py OR reasoning_agent_local.py)       │
│    - Claude API (cloud) OR Ollama (local)                                    │
│    - Analyzes claim vs evidence                                             │
│    - Outputs: verdict, confidence, supporting/contradicting spans           │
└─────────────────────┬───────────────────────────────┬───────────────────────┘
                      │                               │
                      ▼                               ▼
┌─────────────────────────────┐    ┌─────────────────────────────┐
│    AGENT 6: DOSSIER         │    │    AGENT 7: AGGREGATOR      │
│    (dossier_writer.py)      │    │    (results_aggregator.py)  │
│    - Markdown reports       │    │    - KDSH format CSV        │
│    - Human-readable         │    │    - Story ID, Prediction,  │
│                             │    │      Rationale              │
└─────────────────────────────┘    └─────────────────────────────┘

================================================================================
SECTION 3: DIRECTORY STRUCTURE
================================================================================

StrawHats_KDSH_2026/
├── agents/                      # Pipeline agents (7 total)
│   ├── ingestion_agent.py       # Chunks novels (Pathway integration)
│   ├── embedding_agent.py       # Creates FAISS vector index
│   ├── claim_parser.py          # Parses CSV claims
│   ├── retriever_agent.py       # RAG retrieval
│   ├── reasoning_agent.py       # Claude API reasoning
│   ├── reasoning_agent_local.py # Ollama local reasoning
│   ├── dossier_writer.py        # Markdown reports
│   └── results_aggregator.py    # Final CSV output
├── data/
│   ├── novels/                  # Novel .txt files
│   ├── train.csv                # Training claims with labels
│   └── test.csv                 # Test claims to verify
├── flask_api/                   # Dashboard backend
│   ├── app.py                   # Main API server
│   ├── claims.py                # Claims management
│   ├── upload.py                # File upload
│   └── history.py               # Run history
├── frontend/                    # React dashboard
├── run_all.py                   # Pipeline orchestrator
├── create_submission.py         # Creates submission ZIP
└── requirements.txt             # Python dependencies

GENERATED DIRECTORIES (after running pipeline):
├── chunks/                      # Chunked novel text (JSONL)
├── index/                       # FAISS index + metadata
├── claims/                      # Parsed claims (JSONL)
├── evidence/                    # Retrieved passages per claim
├── verdicts/                    # LLM verdicts (JSON)
├── dossiers/                    # Markdown reports
└── output/
    ├── results.csv              # KDSH submission format
    └── results_extended.csv     # Extended format for dashboard

================================================================================
SECTION 4: HOW THE PIPELINE WORKS
================================================================================

STEP 1: INGESTION (ingestion_agent.py)
----------------------------------------
- Reads all .txt files from data/novels/
- Uses Pathway framework for document loading (Track A requirement)
- Chunks each novel into segments:
  - Chunk size: 1400 tokens
  - Overlap: 300 tokens (preserves context across chunks)
- Output: chunks/chunks.jsonl

STEP 2: EMBEDDING (embedding_agent.py)
----------------------------------------
- Loads chunks from chunks.jsonl
- Uses sentence-transformers model (all-MiniLM-L6-v2)
- Creates 384-dimensional embeddings for each chunk
- Builds FAISS index for fast similarity search
- Output: index/faiss.index, index/meta.pkl

STEP 3: CLAIM PARSING (claim_parser.py)
----------------------------------------
- Reads data/train.csv and data/test.csv
- Extracts claims with: id, book_name, character, content
- Output: claims/claims.jsonl

STEP 4: RETRIEVAL (retriever_agent.py)
----------------------------------------
- For each claim:
  - Embeds the claim text
  - Searches FAISS index for similar chunks
  - Returns top-k (default 10) most relevant passages
- Output: evidence/{claim_id}.json

STEP 5: REASONING (reasoning_agent.py or reasoning_agent_local.py)
-------------------------------------------------------------------
- For each claim + evidence:
  - Sends to LLM (Claude API or Ollama)
  - Prompts LLM to determine: SUPPORTED or CONTRADICTED
  - Returns: verdict, confidence, reasoning
- Output: verdicts/{claim_id}.json

STEP 6: DOSSIER WRITING (dossier_writer.py)
--------------------------------------------
- Creates human-readable Markdown reports
- Links evidence to claims
- Output: dossiers/{claim_id}.md

STEP 7: RESULTS AGGREGATION (results_aggregator.py)
----------------------------------------------------
- Collects all verdicts
- Maps: supported -> 1, contradicted -> 0
- Creates KDSH-compliant CSV
- Output: output/results.csv

================================================================================
SECTION 5: HOW TO RUN
================================================================================

OPTION 1: Full Pipeline with Claude API
----------------------------------------
python run_all.py

OPTION 2: Full Pipeline with Local LLM (Ollama)
------------------------------------------------
python run_all.py --local

OPTION 3: Clean Run (removes old results)
------------------------------------------
python run_all.py --local --clean

OPTION 4: Run Specific Stage
-----------------------------
python run_all.py --start-from reasoning

OPTION 5: Skip Reasoning (use cached verdicts)
-----------------------------------------------
python run_all.py --skip-reasoning

================================================================================
SECTION 6: PATHWAY INTEGRATION (Track A Requirement)
================================================================================

Pathway is integrated in: agents/ingestion_agent.py

The ingestion agent:
1. Imports Pathway framework
2. Logs Pathway availability status
3. Can use Pathway for document loading

Current status: Pathway is INSTALLED and available

When you run the pipeline, you should see:
"✓ Pathway framework detected - Track A compliant"

================================================================================
SECTION 7: OUTPUT FORMAT (KDSH Submission)
================================================================================

The results.csv file follows EXACTLY this format:

Story ID,Prediction,Rationale
95,1,Evidence confirms the claim about Villefort...
136,0,The backstory contradicts the novel's timeline...

Where:
- Story ID: The claim ID from test.csv
- Prediction: 1 (consistent) or 0 (contradicted)
- Rationale: 1-2 line explanation

================================================================================
SECTION 8: SUBMISSION PACKAGE
================================================================================

Run: python create_submission.py

This creates: StrawHats_KDSH_2026.zip containing:
- All source code (agents/, flask_api/, frontend/)
- Configuration files (requirements.txt, run_all.py, etc.)
- results.csv (KDSH format)

STILL NEEDED:
- Report PDF (max 10 pages) describing approach

================================================================================
SECTION 9: CURRENT ISSUES / NOTES
================================================================================

ISSUE 1: Old Results Persisting
-------------------------------
SOLUTION: Use --clean flag to remove old results
Command: python run_all.py --local --clean

ISSUE 2: Processing Wrong Claims
---------------------------------
The pipeline was processing old evidence files from previous runs.
SOLUTION: The --clean flag now removes all intermediate directories.

ISSUE 3: Model Predicting All "Supported"
------------------------------------------
The local LLM (Ollama) may be biased toward "supported" verdicts.
This could happen if:
- Evidence retrieved is always positive
- LLM prompt needs refinement
- Need to calibrate confidence thresholds

================================================================================
SECTION 10: WHAT MAKES THIS TRACK A COMPLIANT
================================================================================

✓ Uses Pathway framework for document ingestion
✓ Uses RAG (Retrieval-Augmented Generation) pipeline
✓ Uses LLM for reasoning (Claude or Ollama)
✓ Handles long context via chunking + overlap
✓ Produces structured evidence rationale
✓ Outputs KDSH-compliant results.csv

================================================================================
SECTION 11: TECHNOLOGIES USED
================================================================================

PYTHON PACKAGES:
- pathway: Document processing framework (Track A requirement)
- faiss-cpu: Vector similarity search
- sentence-transformers: Text embeddings
- anthropic: Claude API client
- tiktoken: Token counting
- flask: Web API framework
- flask-cors: CORS support
- pandas: Data manipulation

FRONTEND:
- React 18
- Vite
- Tailwind CSS

LLM OPTIONS:
- Claude API (claude-3-5-sonnet-20241022)
- Ollama (mistral, phi3, llama3.2)

================================================================================
SECTION 12: FILE DESCRIPTIONS
================================================================================

run_all.py
----------
Main pipeline orchestrator. Runs all 7 agents in sequence.
Flags: --local, --clean, --test-mode, --skip-reasoning, --start-from

create_submission.py
--------------------
Creates KDSH submission ZIP file with code and results.

agents/ingestion_agent.py
-------------------------
Chunks novels into overlapping segments. Uses Pathway for Track A.

agents/embedding_agent.py
-------------------------
Creates FAISS vector index from chunks.

agents/claim_parser.py
----------------------
Parses train.csv and test.csv into claims.jsonl.

agents/retriever_agent.py
-------------------------
Retrieves relevant passages for each claim using FAISS.

agents/reasoning_agent.py
-------------------------
Uses Claude API to determine verdict for each claim.

agents/reasoning_agent_local.py
-------------------------------
Uses Ollama (local LLM) to determine verdict for each claim.

agents/dossier_writer.py
------------------------
Generates Markdown reports linking evidence to claims.

agents/results_aggregator.py
---------------------------
Compiles all verdicts into KDSH-format results.csv.

================================================================================
END OF DOCUMENTATION
================================================================================
